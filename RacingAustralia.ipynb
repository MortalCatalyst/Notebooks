{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saturday, 26 June 2021\n",
      "Saturday, 19 June 2021\n",
      "Saturday, 05 June 2021\n"
     ]
    }
   ],
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import re\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "def getRequest(url):\r\n",
    "    try:\r\n",
    "        return BeautifulSoup(requests.get(url).content, \"html.parser\")        \r\n",
    "    except Exception as e:\r\n",
    "        print(\"The URL failed to return\")\r\n",
    "        print(getattr(e, 'message', repr(e)))\r\n",
    "       \r\n",
    "\r\n",
    "url = \"https://www.racingaustralia.horse\"\r\n",
    "main_page_url = \"https://www.racingaustralia.horse/home.aspx\"\r\n",
    "\r\n",
    "# results = [i['href'] for i in soup.select('[href*=Calendar_Results]')]\r\n",
    "\r\n",
    "results = [i['href'] for i in getRequest(main_page_url).select('[href*=Calendar_Results]')]\r\n",
    "    \r\n",
    "stateResults = []\r\n",
    "\r\n",
    "for item in results:\r\n",
    "    state = url + item\r\n",
    "    stateResults.append(\"%s\" % state)\r\n",
    "    \r\n",
    "# print(stateResults)\r\n",
    "# # \r\n",
    "#TODO Review option to use /InteractiveForm/TrackCondition.aspx?State=NSW\r\n",
    "# interim = stateResults[0]\r\n",
    "gardens = []\r\n",
    "\r\n",
    "#TODO determine track names to retrieve URLS (endswith)\r\n",
    "for item in stateResults:\r\n",
    "    for state_url in getRequest(item).findAll('a'):\r\n",
    "        if state_url.attrs['href'].endswith('Gardens'):\r\n",
    "            gardens.append(f\"{url}{state_url['href']}\")\r\n",
    "\r\n",
    "# removes list duplicates\r\n",
    "url_list = list(dict.fromkeys(gardens))\r\n",
    "# Gets Meeting Dates.\r\n",
    "for item in url_list:\r\n",
    "    try:\r\n",
    "        print(getRequest(item).find('span').text)\r\n",
    "    except:\r\n",
    "        print(f\"maybe a bad url {getRequest(item)}\")\r\n",
    "\r\n",
    "keys = []\r\n",
    "values = []\r\n",
    "\r\n",
    "def find_by_label(soup, label):\r\n",
    "    return soup.find(\"b\", text=re.compile(label)).next_sibling\r\n",
    "\r\n",
    "output = []\r\n",
    "\r\n",
    "def getStats(urls, len_url_list):\r\n",
    "    while len_url_list > 0:\r\n",
    "        for url in urls:\r\n",
    "            stats = getRequest(url).find('div', {'class': 'race-venue-bottom'})\r\n",
    "            for data in stats.findAll('b'):\r\n",
    "                keys.append(data.text)\r\n",
    "            for key in keys:\r\n",
    "                values.append(find_by_label(stats, key).strip()) \r\n",
    "        len_url_list =  len_url_list - 1\r\n",
    "        yield keys, values\r\n",
    "\r\n",
    "for k, v in getStats(url_list, len(url_list)):\r\n",
    "    output.append(dict(zip(k, v)))\r\n",
    "    \r\n",
    "# print(output)\r\n",
    "\r\n",
    "# dataframe = pd.read_csv('C:\\Users\\sayth\\Documents\\Data')\r\n",
    "df = pd.DataFrame(output)\r\n",
    "df.to_csv(r'C:\\Users\\sayth\\Documents\\Data\\track_data.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de2fbdc5ea4bcabd9c94e1b89c75e2f585cef33d261b99650aa271a24d900136"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('WebScraping': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}